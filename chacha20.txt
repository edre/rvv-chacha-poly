// This sketches out a vectorization of chacha20 for the RiscV Vector extension.
// The basic idea is in https://eprint.iacr.org/2013/759.pdf

// Using 8 vector registers, allowing LMUL=4
// v0 = first state row
// v1 = second state row
// v2 = third state row
// v3 = fourth state row
// v4 = tmp
// v5 = const indexes for word rotate 1
// v6 = const indexes for word rotate 2
// v7 = const indexes for word rotate 3
//
// Either we need 4 more registers to cache the initial state for final
// adding, or we need to write the initial state to memory. This would be half
// the length of the total vector register. It's unclear whether needing to
// restore from memory in the outer loop is worth the gain of bumping LMUL
// from 2 to 4. Probably implementation dependent.

// Not super efficient, but only needs to be done a few times at function init
def write128andSplat(vd, bytes):
  setvl SEW=32, LMUL=4
  vmv.s.x vd word3; vslide1up vd;
  vmv.s.x vd word2; vslide1up vd;
  vmv.s.x vd word1; vslide1up vd;
  vmv.s.x vd word0;
  // splat to all other 128bit slots
  setvl SEW=128, LMUL=4
  vrgather.vi vd, vd, 0

//// Function initialization. Initialize starting state and const vectors.

// Initialize const vectors
// const load first 16 bytes, rangesum by 4s
v5 = write128andSplat((1,2,3,0));
v6 = write128andSplat((2,3,0,1));
v7 = write128andSplat((3,0,1,2));
// set the tmp register to (0,0,0,0,4,4,4,4,8,8,8,8,...) for offsets
setvl SEW=32, LMUL=4
v4 = vid();
v4 &= ^3;
v5 += v4;
v6 += v4;
v7 += v4;

// initialize state, using a 128-bit type for state initialization
setvl SEW=128, LMUL=4
// Load 128 bit constant to v0
v0 = write128andSplat("expand 32-byte k");
// Load 128 bit first keyhalf to v1; splat to whole vector.
v1 = write128andSplat(&key);
// Load 128 bit second keyhalf to v2; splat to whole vector.
v2 = write128andSplat(&key+16);
// Load nonce and counter to v3; splat then rangesum counter.
v3 = write128andSplat((initCounter,nonce));
// Prepare counter
v4 = vid(); // 128bit
setvl SEW=64, LMUL=4
v3 += v4 // 64 bit addition only on the counter

//// Cache initial state, either in memory or other vector registers
v8 = v0; v9 = v1; v10 = v2; v11 = v3;

// Outer loop, processing N chachas at a time, or 64N bytes of input
// Input must be pre-padded to a multiple of 64.
while still bytes left {

// Core hash function: 32 bit int mode
setvl remainingInput SEW=32, LMUL=4

loop 10 times {
// Double round
// Each round is identical apart from the gather args.
// In the unlikely case we're optimizing for code size,
// these could be folded together with an additional 3-instruction xorswap(v5,v7) in each loop
v0 += v1; v3 ^= v0;
v4 = v3 << 16; v3 >>= 16; v3 ^= v4; // v3 <<<= 16;
v2 += v3; v1 ^= v2; v1 <<<= 12;
v0 += v1; v3 ^= v0; v3 <<<= 8;
v2 += v3; v1 ^= v2; v1 <<<= 7;

v1 = vrgather.vv(v1, v5);
v2 = vrgather.vv(v2, v6);
v3 = vrgather.vv(v3, v7);

v0 += v1; v3 ^= v0; v3 <<<= 16;
v2 += v3; v1 ^= v2; v1 <<<= 12;
v0 += v1; v3 ^= v0; v3 <<<= 8;
v2 += v3; v1 ^= v2; v1 <<<= 7;

v1 = vrgather.vv(v1, v7);
v2 = vrgather.vv(v2, v6);
v3 = vrgather.vv(v3, v5);
}

// Add initial state back in.
v0 += v8; v1 += v9; v2 += v10; v3 += v11;

// Xor plaintext to ciphertext or vice versa.
setvl SEW=128
// strided load to v4, xor,
v4 = loadstride(*x); v4 ^= v0; storestride(*x);
x += 16;
v4 = loadstride(*x); v4 ^= v1; storestride(*x);
x += 16;
v4 = loadstride(*x); v4 ^= v2; storestride(*x);
x += 16;
v4 = loadstride(*x); v4 ^= v3; storestride(*x);
x += vl - 48;

// Restore initial state and bump counters.
v0 = v8; v1 = v9; v2 = v10; v3 = v11;
v4 = write128andSplat(vl128);
v3 += v4;

}
